{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e244c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e5373a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 09:57:28.268819: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-19 09:57:28.300908: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 09:57:28.796200: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Version:  2.12.0\n",
      "Python Version:  3.10.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print('TF Version: ', tf.__version__)\n",
    "from platform import python_version\n",
    "print('Python Version: ', python_version())\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f8a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = 3 # define the GPU to use\n",
    "# Set the GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU)\n",
    "#from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cae147b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ciaran/.local/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/ciaran/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-14 15:47:56.171520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22071 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:4e:00.0, compute capability: 8.9\n",
      "/home/ciaran/.local/lib/python3.10/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "## Setup\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "autotune = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Prepare the dataset\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Load the horse-zebra dataset using tensorflow-datasets.\n",
    "#dataset, _ = tfds.load(\"cycle_gan/horse2zebra\", with_info=True, as_supervised=True)\n",
    "noisy_tr = np.load('hn_train_set.npy')\n",
    "clean_tr = np.load('ln_train_set.npy')\n",
    "noisy_va = np.load('hn_valid_set.npy')\n",
    "clean_va = np.load('ln_valid_set.npy')\n",
    "\n",
    "noisy_va_sup = np.load('hn_valid_set_sup.npy')\n",
    "clean_va_sup = np.load('ln_valid_set_sup.npy')\n",
    "\n",
    "noisy_te = np.load('hn_test_set.npy')\n",
    "clean_te = np.load('ln_test_set.npy')\n",
    "#train_noisy, train_clean = np.expand_dims(noisy_tr[:200],axis=0), np.expand_dims(clean_tr[:200],axis=0)\n",
    "#test_noisy, test_clean = np.expand_dims(noisy_tr[:1],axis=0), np.expand_dims(clean_tr[:1],axis=0)\n",
    "train_noisy, train_clean = noisy_tr, clean_tr\n",
    "valid_noisy, valid_clean = noisy_va, clean_va\n",
    "valid_noisy_sup, valid_clean_sup = noisy_va_sup, clean_va_sup\n",
    "test_noisy, test_clean = noisy_te, clean_te\n",
    "\n",
    "\n",
    "\n",
    "input_spec_size = (500,1)\n",
    "\n",
    "buffer_size = 256\n",
    "batch_size = 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_AE(\n",
    "    name=None,\n",
    "):\n",
    "    spec_input = layers.Input(shape=input_spec_size, name=name + \"_spec_input\")\n",
    "    x1 = layers.ZeroPadding1D(padding=6)(spec_input)\n",
    "    x1 = layers.Conv1D(64,7, activation='relu', padding='same')(x1)\n",
    "    #x1 = layers.Conv1D(32,3, activation='relu', padding='same')(x1)\n",
    "    x2 = layers.MaxPooling1D(2)(x1)\n",
    "    x3 = layers.Conv1D(32,7, activation='relu', padding='same')(x2)\n",
    "    #x3 = layers.Conv1D(32,3, activation='relu', padding='same')(x3)\n",
    "    x4 = layers.MaxPooling1D(2)(x3)\n",
    "    x5 = layers.Conv1D(16,7, activation='relu', padding='same')(x4)\n",
    "    #x5 = layers.Conv1D(32,3, activation='relu', padding='same')(x5)\n",
    "    x6 = layers.MaxPooling1D(2)(x5)\n",
    "    x7 = layers.Conv1D(8,7, activation='relu', padding='same')(x6)\n",
    "    x7 = layers.MaxPooling1D(2)(x7)\n",
    "    x7 = layers.Conv1D(1,7, activation='relu', padding='same')(x7)\n",
    "    #x7 = layers.Conv1D(16,3, activation='relu', padding='same')(x7)\n",
    "    d4 = layers.UpSampling1D(2)(x7)\n",
    "    d5 = layers.Conv1D(8,7,strides=1, activation='relu', padding='same')(d4)\n",
    "    d5 = layers.UpSampling1D(2)(d5)\n",
    "    d5 = layers.Conv1D(16,7,strides=1, activation='relu', padding='same')(d5)\n",
    "    #d5 = layers.Conv1D(32,1,strides=1, activation='relu', padding='same')(d5)\n",
    "    d6 = layers.UpSampling1D(2)(d5)\n",
    "    d7 = layers.Conv1D(32,7,strides=1, activation='relu', padding='same')(d6)\n",
    "    #d7 = layers.Conv1D(32,1,strides=1, activation='relu', padding='same')(d7)\n",
    "    d8 = layers.UpSampling1D(2)(d7)\n",
    "    d9 = layers.Conv1D(64,7,strides=1, activation='relu', padding='same')(d8)\n",
    "    #d9 = layers.Conv1D(32,1,strides=1, activation='relu', padding='same')(d9)\n",
    "    d9 = layers.Cropping1D(6)(d9)\n",
    "    decoded = layers.Conv1D(1,1,strides=1, padding='same')(d9)\n",
    "    decoded = layers.LeakyReLU(alpha=0.3)(decoded)\n",
    "    model = keras.models.Model(spec_input, decoded, name=name)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Get the model\n",
    "net_AE = get_AE(name=\"AE\")\n",
    "\n",
    "\"\"\"\n",
    "## Build the model\n",
    "\n",
    "We will override the `train_step()` method of the `Model` class\n",
    "for training via `fit()`.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class AE(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        network_AE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.net_AE = network_AE\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return (\n",
    "            self.net_AE(inputs),\n",
    "\n",
    "        )\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        net_AE_optimizer,\n",
    "        AE_loss_fn,\n",
    "\n",
    "    ):\n",
    "        super().compile()\n",
    "        self.net_AE_optimizer = net_AE_optimizer\n",
    "        self.AE_loss_fn = AE_loss_fn\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        # x is noisy and y is high SNR\n",
    "        spec_x, spec_y = batch_data\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # denoise spectra\n",
    "            denoised_y = self.net_AE(spec_x, training=True)\n",
    "            \n",
    "            # AE adverserial loss\n",
    "            net_AE_loss = self.AE_loss_fn(denoised_y,spec_y)\n",
    "            \n",
    "            # Total AE loss\n",
    "            total_loss_AE = net_AE_loss \n",
    "\n",
    "\n",
    "\n",
    "        # Get the gradients for the model\n",
    "        grads_AE = tape.gradient(total_loss_AE, self.net_AE.trainable_variables)\n",
    "\n",
    "\n",
    "        # Update the weights of the model\n",
    "        self.net_AE_optimizer.apply_gradients(\n",
    "            zip(grads_AE, self.net_AE.trainable_variables)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"AE_loss\": total_loss_AE,\n",
    "        }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Create a callback that periodically saves spectra\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate and save spectra after each epoch\"\"\"\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        #manually batch test set (200), and evaluate them\n",
    "        #save noisy network inputs, denoised spectra\n",
    "        #and ground truths\n",
    "\n",
    "        spectra = test_noisy\n",
    "        prediction = np.zeros(np.shape(spectra))\n",
    "        GTS = np.zeros(np.shape(spectra))\n",
    "        inputs = np.zeros(np.shape(spectra))\n",
    "        counter = 0\n",
    "        for i in range(200, np.shape(spectra)[0], 200):\n",
    "            prediction[i-200:i,:] = np.squeeze(self.model.net_AE(spectra[i-200:i]))\n",
    "            GTS[i-200:i,:] = test_clean[i-200:i]\n",
    "            inputs[i-200:i,:] = spectra[i-200:i]\n",
    "            counter = counter+1\n",
    "        # get remaining bit of last batch\n",
    "        prediction[(200*counter):] = np.squeeze(self.model.net_AE(spectra[(200*counter):]))\n",
    "        GTS[(200*counter):] = test_clean[(200*counter):]\n",
    "        inputs[(200*counter):] = spectra[(200*counter):]\n",
    "        \n",
    "        prediction = np.reshape(prediction,(-1,500))\n",
    "        GTS = np.reshape(GTS,(-1,500))\n",
    "        inputs = np.reshape(inputs,(-1,500))\n",
    "        path = './epoch_' + str(epoch)\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "        os.mkdir(path)\n",
    "        np.save(path + '/AE_denoised', prediction)\n",
    "        np.save(path + '/AE_denoised_GT', GTS)\n",
    "        np.save(path + '/AE_input', inputs)\n",
    "\n",
    "        \n",
    "        \n",
    "        # compute unsupervised validation loss\n",
    "        spectra_valid = valid_noisy\n",
    "        prediction_valid = np.zeros(np.shape(spectra_valid))\n",
    "        GTS_valid = np.zeros(np.shape(spectra_valid))\n",
    "        inputs_valid = np.zeros(np.shape(spectra_valid))\n",
    "        \n",
    "        counter = 0\n",
    "        for i in range(200, np.shape(spectra_valid)[0], 200):\n",
    "            prediction_valid[i-200:i,:] = np.squeeze(self.model.net_AE(spectra_valid[i-200:i]))\n",
    "            GTS_valid[i-200:i,:] = valid_clean[i-200:i]\n",
    "            inputs_valid[i-200:i,:] = spectra_valid[i-200:i]\n",
    "            counter = counter+1\n",
    "        # get remaining bit of last batch\n",
    "        prediction_valid[(200*counter):] = np.squeeze(self.model.net_AE(spectra_valid[(200*counter):]))\n",
    "        GTS_valid[(200*counter):] = valid_clean[(200*counter):]\n",
    "        inputs_valid[(200*counter):] = spectra_valid[(200*counter):]\n",
    "        \n",
    "        prediction_valid = np.reshape(prediction_valid,(-1,500))\n",
    "        GTS_valid = np.reshape(GTS_valid,(-1,500))\n",
    "        inputs_valid = np.reshape(inputs_valid,(-1,500))\n",
    "        \n",
    "        cluster_true = KMeans(8, random_state=4).fit(GTS_valid)\n",
    "        cluster_pred = cluster_true.predict(prediction_valid)\n",
    "        print(np.shape(cluster_pred))\n",
    "        \n",
    "        cluster_true = cluster_true.labels_\n",
    "        #cluster_pred = cluster_pred.labels_\n",
    "        valid_loss = metrics.calinski_harabasz_score(prediction_valid, cluster_pred)\n",
    "        valid_loss_rand = adjusted_rand_score(cluster_true,cluster_pred)\n",
    "        np.save(path + '/AE_valid_loss_' + str(epoch), valid_loss)\n",
    "        np.save(path + '/AE_valid_loss_rand_' + str(epoch), valid_loss_rand)\n",
    "        \n",
    "        # compute supervised validation loss\n",
    "        spectra_valid_sup = valid_noisy_sup\n",
    "        prediction_valid_sup = np.zeros(np.shape(spectra_valid_sup))\n",
    "        GTS_valid_sup = np.zeros(np.shape(spectra_valid_sup))\n",
    "        inputs_valid_sup = np.zeros(np.shape(spectra_valid_sup))\n",
    "        \n",
    "        counter = 0\n",
    "        for i in range(200, np.shape(spectra_valid_sup)[0], 200):\n",
    "            prediction_valid_sup[i-200:i,:] = np.squeeze(self.model.net_AE(spectra_valid_sup[i-200:i]))\n",
    "            GTS_valid_sup[i-200:i,:] = valid_clean_sup[i-200:i]\n",
    "            inputs_valid_sup[i-200:i,:] = spectra_valid_sup[i-200:i]\n",
    "            counter = counter+1\n",
    "        # get remaining bit of last batch\n",
    "        prediction_valid_sup[(200*counter):] = np.squeeze(self.model.net_AE(spectra_valid_sup[(200*counter):]))\n",
    "        GTS_valid_sup[(200*counter):] = valid_clean_sup[(200*counter):]\n",
    "        inputs_valid_sup[(200*counter):] = spectra_valid_sup[(200*counter):]\n",
    "        \n",
    "        prediction_valid_sup = np.reshape(prediction_valid_sup,(-1,500))\n",
    "        GTS_valid_sup = np.reshape(GTS_valid_sup,(-1,500))\n",
    "        inputs_valid_sup = np.reshape(inputs_valid_sup,(-1,500))\n",
    "        \n",
    "        valid_loss_sup = np.mean(np.mean((np.squeeze(prediction_valid_sup) - np.squeeze(GTS_valid_sup))**2,axis=1))\n",
    "        valid_loss_trad_AE = np.mean(np.mean((np.squeeze(prediction_valid_sup) - np.squeeze(inputs_valid_sup))**2,axis=1))\n",
    "        np.save(path + '/AE_valid_loss_sup_' + str(epoch), valid_loss_sup)\n",
    "        np.save(path + '/valid_loss_trad_AE_' + str(epoch), valid_loss_trad_AE)\n",
    "\n",
    "\"\"\"\n",
    "## Train the end-to-end model\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Loss function for evaluating adversarial loss\n",
    "adv_loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define the loss function for the generators\n",
    "\n",
    "\n",
    "def AE_loss_fn(fake,real):\n",
    "    fake_loss = adv_loss_fn(fake, real)\n",
    "    return fake_loss\n",
    "\n",
    "\n",
    "\n",
    "# Create cycle gan model\n",
    "AE_model = AE(\n",
    "    network_AE=net_AE\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "AE_model.compile(\n",
    "    net_AE_optimizer=keras.optimizers.legacy.Adam(learning_rate=1e-4, beta_1=0.5),\n",
    "    AE_loss_fn=AE_loss_fn,\n",
    ")\n",
    "# Callbacks\n",
    "plotter = GANMonitor()\n",
    "checkpoint_filepath = \"./model_checkpoints/cyclegan_checkpoints.{epoch:03d}\"\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath, save_weights_only=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d99f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176bdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a33709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7170b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 15:47:57.953546: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_5' with dtype double and shape [68925,500]\n",
      "\t [[{{node Placeholder/_5}}]]\n",
      "2023-05-14 15:47:57.953646: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_5' with dtype double and shape [68925,500]\n",
      "\t [[{{node Placeholder/_5}}]]\n",
      "2023-05-14 15:48:19.047940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7530/13785 [===============>..............] - ETA: 5:26 - G_loss: 0.3254 - F_loss: 0.4121 - D_X_loss: 0.2510 - D_Y_loss: 0.2450"
     ]
    }
   ],
   "source": [
    "train_noisy = tf.data.Dataset.from_tensor_slices((train_noisy))\n",
    "train_clean = tf.data.Dataset.from_tensor_slices((train_clean))\n",
    "#test_horses = tf.data.Dataset.from_tensor_slices((test_horses))\n",
    "#test_zebras = tf.data.Dataset.from_tensor_slices((test_zebras))\n",
    "train_noisy = train_noisy.batch(batch_size)\n",
    "train_clean = train_clean.batch(batch_size)\n",
    "\n",
    "\n",
    "\n",
    "history = AE_model.fit(\n",
    "    tf.data.Dataset.zip((train_noisy, train_noisy)),\n",
    "    epochs=100,\n",
    "    callbacks=[plotter, model_checkpoint_callback],\n",
    ")\n",
    "\n",
    "np.save('my_history.npy',history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041d8d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#history=np.load('my_history.npy',allow_pickle='TRUE').item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc377b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f31445d7",
   "metadata": {},
   "source": [
    "\n",
    "# Load the checkpoints\n",
    "weight_file = \"./saved_checkpoints/cyclegan_checkpoints.001\"\n",
    "cycle_gan_model.load_weights(weight_file).expect_partial()\n",
    "print(\"Weights loaded successfully\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
