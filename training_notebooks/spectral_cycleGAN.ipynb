{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e244c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code was adapted from:\n",
    "\n",
    "@misc{Nain2020,\n",
    "  author = {Aakash Kumar Nain},\n",
    "  title = {Cycle{GAN}},\n",
    "  year = {2020},\n",
    "  publisher = {GitHub},\n",
    "  journal = {GitHub repository},\n",
    "  howpublished = {\\url{https://github.com/keras-team/keras-io/blob/master/examples/generative/cyclegan.py},note={Accessed on: 26/05/2023} }\n",
    "\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e5373a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 09:57:28.268819: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-19 09:57:28.300908: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 09:57:28.796200: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Version:  2.12.0\n",
      "Python Version:  3.10.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print('TF Version: ', tf.__version__)\n",
    "from platform import python_version\n",
    "print('Python Version: ', python_version())\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f8a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = 2 # define the GPU to use\n",
    "# Set the GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU)\n",
    "#from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cae147b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ciaran/.local/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/ciaran/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-14 15:47:56.171520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22071 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:4e:00.0, compute capability: 8.9\n",
      "/home/ciaran/.local/lib/python3.10/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "## Setup\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "autotune = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Prepare the dataset\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# \n",
    "# \n",
    "noisy_tr = np.load('hn_train_set.npy')\n",
    "clean_tr = np.load('ln_train_set.npy')\n",
    "noisy_va = np.load('hn_valid_set.npy')\n",
    "clean_va = np.load('ln_valid_set.npy')\n",
    "\n",
    "noisy_va_sup = np.load('hn_valid_set_sup.npy')\n",
    "clean_va_sup = np.load('ln_valid_set_sup.npy')\n",
    "\n",
    "noisy_te = np.load('hn_test_set.npy')\n",
    "clean_te = np.load('ln_test_set.npy')\n",
    "#train_noisy, train_clean = np.expand_dims(noisy_tr[:200],axis=0), np.expand_dims(clean_tr[:200],axis=0)\n",
    "#test_noisy, test_clean = np.expand_dims(noisy_tr[:1],axis=0), np.expand_dims(clean_tr[:1],axis=0)\n",
    "train_noisy, train_clean = noisy_tr, clean_tr\n",
    "valid_noisy, valid_clean = noisy_va, clean_va\n",
    "valid_noisy_sup, valid_clean_sup = noisy_va_sup, clean_va_sup\n",
    "test_noisy, test_clean = noisy_te, clean_te\n",
    "\n",
    "\n",
    "\n",
    "input_spec_size = (500,1)\n",
    "# Weights initializer for the layers.\n",
    "kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "# Gamma initializer for instance normalization.\n",
    "gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "buffer_size = 256\n",
    "batch_size = 5\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Building blocks used in the CycleGAN generators and discriminators\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def residual_block(\n",
    "    x,\n",
    "    activation,\n",
    "    kernel_initializer=kernel_init,\n",
    "    kernel_size=(3),\n",
    "    strides=(1),\n",
    "    padding=\"same\",\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    dim = x.shape[-1]\n",
    "    input_tensor = x\n",
    "\n",
    "    #x = ReflectionPadding2D()(input_tensor)\n",
    "    x = layers.Conv1D(\n",
    "        dim,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(input_tensor)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = activation(x)\n",
    "\n",
    "    #x = ReflectionPadding2D()(x)\n",
    "    x = layers.Conv1D(\n",
    "        dim,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = layers.add([input_tensor, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def downsample(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_initializer=kernel_init,\n",
    "    kernel_size=(3),\n",
    "    strides=(2),\n",
    "    padding=\"same\",\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv1D(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def upsample(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3),\n",
    "    strides=(2),\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=kernel_init,\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv1DTranspose(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Build the generators\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_resnet_generator(\n",
    "    filters=64,\n",
    "    num_downsampling_blocks=2,\n",
    "    num_residual_blocks=9,\n",
    "    num_upsample_blocks=2,\n",
    "    gamma_initializer=gamma_init,\n",
    "    name=None,\n",
    "):\n",
    "    spec_input = layers.Input(shape=input_spec_size, name=name + \"_spec_input\")\n",
    "    #x = ReflectionPadding2D(padding=(3, 3))(spec_input)\n",
    "    x = layers.Conv1D(filters, (7), kernel_initializer=kernel_init, use_bias=False,padding=\"same\")(\n",
    "        spec_input\n",
    "    )\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    # Downsampling\n",
    "    for _ in range(num_downsampling_blocks):\n",
    "        filters *= 2\n",
    "        x = downsample(x, filters=filters, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Residual blocks\n",
    "    for _ in range(num_residual_blocks):\n",
    "        x = residual_block(x, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Upsampling\n",
    "    for _ in range(num_upsample_blocks):\n",
    "        filters //= 2\n",
    "        x = upsample(x, filters, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Final block\n",
    "    #x = ReflectionPadding2D(padding=(3))(x)\n",
    "    x = layers.Conv1D(1, (7), padding=\"same\")(x)\n",
    "    x = layers.Activation(\"tanh\")(x)\n",
    "\n",
    "    model = keras.models.Model(spec_input, x, name=name)\n",
    "    return model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Build the discriminators\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_discriminator(\n",
    "    filters=64, kernel_initializer=kernel_init, num_downsampling=3, name=None\n",
    "):\n",
    "    spec_input = layers.Input(shape=input_spec_size, name=name + \"_spec_input\")\n",
    "    x = layers.Conv1D(\n",
    "        filters,\n",
    "        (4),\n",
    "        strides=(2),\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )(spec_input)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    num_filters = filters\n",
    "    for num_downsample_block in range(3):\n",
    "        num_filters *= 2\n",
    "        if num_downsample_block < 2:\n",
    "            x = downsample(\n",
    "                x,\n",
    "                filters=num_filters,\n",
    "                activation=layers.LeakyReLU(0.2),\n",
    "                kernel_size=(4),\n",
    "                strides=(2),\n",
    "            )\n",
    "        else:\n",
    "            x = downsample(\n",
    "                x,\n",
    "                filters=num_filters,\n",
    "                activation=layers.LeakyReLU(0.2),\n",
    "                kernel_size=(4),\n",
    "                strides=(1),\n",
    "            )\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        1, (4), strides=(1), padding=\"same\", kernel_initializer=kernel_initializer\n",
    "    )(x)\n",
    "\n",
    "    model = keras.models.Model(inputs=spec_input, outputs=x, name=name)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get the generators\n",
    "gen_G = get_resnet_generator(name=\"generator_G\")\n",
    "gen_F = get_resnet_generator(name=\"generator_F\")\n",
    "\n",
    "# Get the discriminators\n",
    "disc_X = get_discriminator(name=\"discriminator_X\")\n",
    "disc_Y = get_discriminator(name=\"discriminator_Y\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Build the CycleGAN model\n",
    "\n",
    "We will override the `train_step()` method of the `Model` class\n",
    "for training via `fit()`.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CycleGan(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator_G,\n",
    "        generator_F,\n",
    "        discriminator_X,\n",
    "        discriminator_Y,\n",
    "        lambda_cycle=10.0,\n",
    "        lambda_identity=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gen_G = generator_G\n",
    "        self.gen_F = generator_F\n",
    "        self.disc_X = discriminator_X\n",
    "        self.disc_Y = discriminator_Y\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        self.lambda_identity = lambda_identity\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return (\n",
    "            self.disc_X(inputs),\n",
    "            self.disc_Y(inputs),\n",
    "            self.gen_G(inputs),\n",
    "            self.gen_F(inputs),\n",
    "        )\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        gen_G_optimizer,\n",
    "        gen_F_optimizer,\n",
    "        disc_X_optimizer,\n",
    "        disc_Y_optimizer,\n",
    "        gen_loss_fn,\n",
    "        disc_loss_fn,\n",
    "    ):\n",
    "        super().compile()\n",
    "        self.gen_G_optimizer = gen_G_optimizer\n",
    "        self.gen_F_optimizer = gen_F_optimizer\n",
    "        self.disc_X_optimizer = disc_X_optimizer\n",
    "        self.disc_Y_optimizer = disc_Y_optimizer\n",
    "        self.generator_loss_fn = gen_loss_fn\n",
    "        self.discriminator_loss_fn = disc_loss_fn\n",
    "        self.cycle_loss_fn = keras.losses.MeanAbsoluteError()\n",
    "        self.identity_loss_fn = keras.losses.MeanAbsoluteError()\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        # x is noisy and y is clean\n",
    "        real_x, real_y = batch_data\n",
    "\n",
    "        # For CycleGAN, we need to calculate different\n",
    "        # kinds of losses for the generators and discriminators.\n",
    "        # We will perform the following steps here:\n",
    "        #\n",
    "        # 1. Pass real spectra through the generators and get the generated spectra\n",
    "        # 2. Pass the generated spectra back to the generators to check if we\n",
    "        #    we can predict the original spectra from the generated spectra.\n",
    "        # 3. Do an identity mapping of the real spectra using the generators.\n",
    "        # 4. Pass the generated spectra in 1) to the corresponding discriminators.\n",
    "        # 5. Calculate the generators total loss (adverserial + cycle + identity)\n",
    "        # 6. Calculate the discriminators loss\n",
    "        # 7. Update the weights of the generators\n",
    "        # 8. Update the weights of the discriminators\n",
    "        # 9. Return the losses in a dictionary\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # low snr to fake high snr\n",
    "            fake_y = self.gen_G(real_x, training=True)\n",
    "            # high snr to fake low snr -> y2x\n",
    "            fake_x = self.gen_F(real_y, training=True)\n",
    "\n",
    "            # Cycle (low snr to fake high snr to fake low snr): x -> y -> x\n",
    "            cycled_x = self.gen_F(fake_y, training=True)\n",
    "            # Cycle (high snr to fake low snr to fake high snr) y -> x -> y\n",
    "            cycled_y = self.gen_G(fake_x, training=True)\n",
    "\n",
    "            # Identity mapping\n",
    "            same_x = self.gen_F(real_x, training=True)\n",
    "            same_y = self.gen_G(real_y, training=True)\n",
    "\n",
    "            # Discriminator output\n",
    "            disc_real_x = self.disc_X(real_x, training=True)\n",
    "            disc_fake_x = self.disc_X(fake_x, training=True)\n",
    "\n",
    "            disc_real_y = self.disc_Y(real_y, training=True)\n",
    "            disc_fake_y = self.disc_Y(fake_y, training=True)\n",
    "\n",
    "            # Generator adverserial loss\n",
    "            gen_G_loss = self.generator_loss_fn(disc_fake_y)\n",
    "            gen_F_loss = self.generator_loss_fn(disc_fake_x)\n",
    "\n",
    "            # Generator cycle loss\n",
    "            cycle_loss_G = self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle\n",
    "            cycle_loss_F = self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle\n",
    "\n",
    "            # Generator identity loss\n",
    "            id_loss_G = (\n",
    "                self.identity_loss_fn(real_y, same_y)\n",
    "                * self.lambda_cycle\n",
    "                * self.lambda_identity\n",
    "            )\n",
    "            id_loss_F = (\n",
    "                self.identity_loss_fn(real_x, same_x)\n",
    "                * self.lambda_cycle\n",
    "                * self.lambda_identity\n",
    "            )\n",
    "\n",
    "            # Total generator loss\n",
    "            total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G\n",
    "            total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F\n",
    "\n",
    "            # Discriminator loss\n",
    "            disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)\n",
    "            disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)\n",
    "\n",
    "        # Get the gradients for the generators\n",
    "        grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\n",
    "        grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\n",
    "\n",
    "        # Get the gradients for the discriminators\n",
    "        disc_X_grads = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n",
    "        disc_Y_grads = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n",
    "\n",
    "        # Update the weights of the generators\n",
    "        self.gen_G_optimizer.apply_gradients(\n",
    "            zip(grads_G, self.gen_G.trainable_variables)\n",
    "        )\n",
    "        self.gen_F_optimizer.apply_gradients(\n",
    "            zip(grads_F, self.gen_F.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Update the weights of the discriminators\n",
    "        self.disc_X_optimizer.apply_gradients(\n",
    "            zip(disc_X_grads, self.disc_X.trainable_variables)\n",
    "        )\n",
    "        self.disc_Y_optimizer.apply_gradients(\n",
    "            zip(disc_Y_grads, self.disc_Y.trainable_variables)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"G_loss\": total_loss_G,\n",
    "            \"F_loss\": total_loss_F,\n",
    "            \"D_X_loss\": disc_X_loss,\n",
    "            \"D_Y_loss\": disc_Y_loss,\n",
    "            \"ID_G_loss\": id_loss_G,\n",
    "            \"ID_F_loss\": id_loss_F,\n",
    "            \"Cycle_G_loss\": cycle_loss_G,\n",
    "            \"Cycle_F_loss\": cycle_loss_F,\n",
    "        }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Create a callback that periodically saves generated spectra\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate and save spectra after each epoch\"\"\"\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        #manually batch test set (200), and evaluate them\n",
    "        #save noisy network inputs, denoised spectra\n",
    "        #and ground truths\n",
    "        spectra = test_noisy\n",
    "        prediction = np.zeros(np.shape(spectra))\n",
    "        GTS = np.zeros(np.shape(spectra))\n",
    "        inputs = np.zeros(np.shape(spectra))\n",
    "        counter = 0\n",
    "        for i in range(200, np.shape(spectra)[0], 200):\n",
    "            prediction[i-200:i,:] = np.squeeze(self.model.gen_G(spectra[i-200:i]))\n",
    "            GTS[i-200:i,:] = test_clean[i-200:i]\n",
    "            inputs[i-200:i,:] = spectra[i-200:i]\n",
    "            counter = counter+1\n",
    "        # get remaining bit of last batch\n",
    "        prediction[(200*counter):] = np.squeeze(self.model.gen_G(spectra[(200*counter):]))\n",
    "        GTS[(200*counter):] = test_clean[(200*counter):]\n",
    "        inputs[(200*counter):] = spectra[(200*counter):]\n",
    "        \n",
    "        prediction = np.reshape(prediction,(-1,500))\n",
    "        GTS = np.reshape(GTS,(-1,500))\n",
    "        inputs = np.reshape(inputs,(-1,500))\n",
    "        path = './epoch_' + str(epoch)\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "        os.mkdir(path)\n",
    "        np.save(path + '/network_denoised', prediction)\n",
    "        np.save(path + '/network_denoised_GT', GTS)\n",
    "        np.save(path + '/network_input', inputs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # compute supervised validation loss\n",
    "        spectra_valid = valid_noisy\n",
    "        prediction_valid = np.zeros(np.shape(spectra_valid))\n",
    "        GTS_valid = np.zeros(np.shape(spectra_valid))\n",
    "        inputs_valid = np.zeros(np.shape(spectra_valid))\n",
    "        \n",
    "        counter = 0\n",
    "        for i in range(200, np.shape(spectra_valid)[0], 200):\n",
    "            prediction_valid[i-200:i,:] = np.squeeze(self.model.gen_G(spectra_valid[i-200:i]))\n",
    "            GTS_valid[i-200:i,:] = valid_clean[i-200:i]\n",
    "            inputs_valid[i-200:i,:] = spectra_valid[i-200:i]\n",
    "            counter = counter+1\n",
    "        # get remaining bit of last batch\n",
    "        prediction_valid[(200*counter):] = np.squeeze(self.model.gen_G(spectra_valid[(200*counter):]))\n",
    "        GTS_valid[(200*counter):] = valid_clean[(200*counter):]\n",
    "        inputs_valid[(200*counter):] = spectra_valid[(200*counter):]\n",
    "        \n",
    "        prediction_valid = np.reshape(prediction_valid,(-1,500))\n",
    "        GTS_valid = np.reshape(GTS_valid,(-1,500))\n",
    "        inputs_valid = np.reshape(inputs_valid,(-1,500))\n",
    "        \n",
    "        cluster_true = KMeans(8, random_state=4).fit(GTS_valid)\n",
    "        cluster_pred = cluster_true.predict(prediction_valid)\n",
    "        \n",
    "        cluster_true = cluster_true.labels_\n",
    "        #cluster_pred = cluster_pred.labels_\n",
    "        valid_loss = metrics.calinski_harabasz_score(prediction_valid, cluster_pred)\n",
    "        valid_loss_rand = adjusted_rand_score(cluster_true,cluster_pred)\n",
    "        np.save(path + '/valid_loss_' + str(epoch), valid_loss)\n",
    "        np.save(path + '/valid_loss_rand_' + str(epoch), valid_loss_rand)\n",
    "        \n",
    "        # compute supervised validation loss\n",
    "        spectra_valid_sup = valid_noisy_sup\n",
    "        prediction_valid_sup = np.zeros(np.shape(spectra_valid_sup))\n",
    "        GTS_valid_sup = np.zeros(np.shape(spectra_valid_sup))\n",
    "        inputs_valid_sup = np.zeros(np.shape(spectra_valid_sup))\n",
    "        \n",
    "        counter = 0\n",
    "        for i in range(200, np.shape(spectra_valid_sup)[0], 200):\n",
    "            prediction_valid_sup[i-200:i,:] = np.squeeze(self.model.gen_G(spectra_valid_sup[i-200:i]))\n",
    "            GTS_valid_sup[i-200:i,:] = valid_clean_sup[i-200:i]\n",
    "            inputs_valid_sup[i-200:i,:] = spectra_valid_sup[i-200:i]\n",
    "            counter = counter+1\n",
    "        # get remaining bit of last batch\n",
    "        prediction_valid_sup[(200*counter):] = np.squeeze(self.model.gen_G(spectra_valid_sup[(200*counter):]))\n",
    "        GTS_valid_sup[(200*counter):] = valid_clean_sup[(200*counter):]\n",
    "        inputs_valid_sup[(200*counter):] = spectra_valid_sup[(200*counter):]\n",
    "        \n",
    "        prediction_valid_sup = np.reshape(prediction_valid_sup,(-1,500))\n",
    "        GTS_valid_sup = np.reshape(GTS_valid_sup,(-1,500))\n",
    "        inputs_valid_sup = np.reshape(inputs_valid_sup,(-1,500))\n",
    "        \n",
    "        valid_loss_sup = np.mean(np.mean((np.squeeze(prediction_valid_sup) - np.squeeze(GTS_valid_sup))**2,axis=1))\n",
    "        \n",
    "        np.save(path + '/valid_loss_sup_' + str(epoch), valid_loss_sup)\n",
    "\n",
    "\"\"\"\n",
    "## Train the end-to-end model\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Loss function for evaluating adversarial loss\n",
    "adv_loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define the loss function for the generators\n",
    "\n",
    "\n",
    "def generator_loss_fn(fake):\n",
    "    fake_loss = adv_loss_fn(tf.ones_like(fake), fake)\n",
    "    return fake_loss\n",
    "\n",
    "\n",
    "# Define the loss function for the discriminators\n",
    "def discriminator_loss_fn(real, fake):\n",
    "    real_loss = adv_loss_fn(tf.ones_like(real), real)\n",
    "    fake_loss = adv_loss_fn(tf.zeros_like(fake), fake)\n",
    "    return (real_loss + fake_loss) * 0.5\n",
    "\n",
    "\n",
    "# Create cycle gan model\n",
    "cycle_gan_model = CycleGan(\n",
    "    generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "cycle_gan_model.compile(\n",
    "    gen_G_optimizer=keras.optimizers.legacy.Adam(learning_rate=2e-5, beta_1=0.5),\n",
    "    gen_F_optimizer=keras.optimizers.legacy.Adam(learning_rate=2e-5, beta_1=0.5),\n",
    "    disc_X_optimizer=keras.optimizers.legacy.Adam(learning_rate=2e-5, beta_1=0.5),\n",
    "    disc_Y_optimizer=keras.optimizers.legacy.Adam(learning_rate=2e-5, beta_1=0.5),\n",
    "    gen_loss_fn=generator_loss_fn,\n",
    "    disc_loss_fn=discriminator_loss_fn,\n",
    ")\n",
    "# Callbacks\n",
    "plotter = GANMonitor()\n",
    "checkpoint_filepath = \"./model_checkpoints/cyclegan_checkpoints.{epoch:03d}\"\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath, save_weights_only=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d99f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176bdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a33709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7170b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 15:47:57.953546: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_5' with dtype double and shape [68925,500]\n",
      "\t [[{{node Placeholder/_5}}]]\n",
      "2023-05-14 15:47:57.953646: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_5' with dtype double and shape [68925,500]\n",
      "\t [[{{node Placeholder/_5}}]]\n",
      "2023-05-14 15:48:19.047940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7530/13785 [===============>..............] - ETA: 5:26 - G_loss: 0.3254 - F_loss: 0.4121 - D_X_loss: 0.2510 - D_Y_loss: 0.2450"
     ]
    }
   ],
   "source": [
    "train_noisy = tf.data.Dataset.from_tensor_slices((train_noisy))\n",
    "train_clean = tf.data.Dataset.from_tensor_slices((train_clean))\n",
    "# \n",
    "# \n",
    "train_noisy = train_noisy.batch(batch_size)\n",
    "train_clean = train_clean.batch(batch_size)\n",
    "\n",
    "\n",
    "\n",
    "history = cycle_gan_model.fit(\n",
    "    tf.data.Dataset.zip((train_noisy, train_clean)),\n",
    "    epochs=50,\n",
    "    callbacks=[plotter, model_checkpoint_callback],\n",
    ")\n",
    "\n",
    "np.save('my_history.npy',history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041d8d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#history=np.load('my_history.npy',allow_pickle='TRUE').item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc377b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f31445d7",
   "metadata": {},
   "source": [
    "\n",
    "# Load the checkpoints\n",
    "weight_file = \"./saved_checkpoints/cyclegan_checkpoints.001\"\n",
    "cycle_gan_model.load_weights(weight_file).expect_partial()\n",
    "print(\"Weights loaded successfully\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
