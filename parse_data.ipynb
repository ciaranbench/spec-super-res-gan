{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc58bb0",
   "metadata": {},
   "source": [
    "# Parsing training set\n",
    "\n",
    "https://github.com/conor-horgan/DeepeR has a link to a dataset of spectra. \n",
    "\n",
    "The database is composed of paired examples (noisy and high SNR versions of the same spectra)\n",
    "\n",
    "Here, we parse this dataset to train our unsupervised spectral denoising framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4bfac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .mat files are nto in hdf5 format for some reason, so we use scipy.io to read them...\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import random\n",
    "\n",
    "#set how much of the training data to split for train and valid\n",
    "train_split = 0.8\n",
    "validation_split = 0.2\n",
    "assert((train_split + validation_split) < 1)\n",
    "#use this if we'd like plit across the combined set formed by their train/test files\n",
    "#test_split = 1 - (train_split + validation_split)\n",
    "\n",
    "hn_train = scipy.io.loadmat('Train_Inputs.mat')\n",
    "hn_train = hn_train['Train_Inputs']\n",
    "ln_train = scipy.io.loadmat('Train_Outputs.mat')\n",
    "ln_train = ln_train['Train_Outputs']\n",
    "\n",
    "hn_test = scipy.io.loadmat('Test_Inputs.mat')\n",
    "hn_test = hn_test['Test_Inputs']\n",
    "ln_test = scipy.io.loadmat('Test_Outputs.mat')\n",
    "ln_test = ln_test['Test_Outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c3b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this if we'd like plit across the combined set formed by their train/test files\n",
    "#hn_all = np.concatenate((hn_train,hn_test), axis=0)\n",
    "#ln_all = np.concatenate((ln_train,ln_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b2d4850",
   "metadata": {},
   "outputs": [],
   "source": [
    "hn_all = hn_train\n",
    "ln_all = ln_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec91e820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12694, 500)\n",
      "(159618, 500)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c54fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "randomised_indices = random.sample(range(0, np.round(np.shape(hn_all)[0])), int(np.floor(np.shape(hn_all)[0])))\n",
    "train_indices = randomised_indices[:int(np.round(train_split*np.shape(hn_all)[0]))]\n",
    "valid_indices = randomised_indices[int(np.round(train_split*np.shape(hn_all)[0])):(int(np.round(train_split*np.shape(hn_all)[0]))+int(np.round(validation_split*np.shape(hn_all)[0])))]\n",
    "#use this if we'd like plit across the combined set formed by their train/test files\n",
    "#test_indices = randomised_indices[(int(np.round(train_split*np.shape(hn_all)[0]))+int(np.round(validation_split*np.shape(hn_all)[0]))):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba8c40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hn_train_set = hn_all[train_indices[:int(round(len(train_indices)/2))]]\n",
    "ln_train_set = ln_all[train_indices[int(round(len(train_indices)/2)):]]\n",
    "\n",
    "#validation set and test set is paired data\n",
    "hn_valid_set = hn_all[valid_indices]\n",
    "ln_valid_set = ln_all[valid_indices]\n",
    "#We just use theor original test set as our test set\n",
    "hn_test_set = hn_all[test_indices]\n",
    "ln_test_set = ln_all[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a7d3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for now, we just use the etst set as validation... but this will change in the future. \n",
    "hn_valid_set = hn_test\n",
    "ln_valid_set = ln_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0325945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('hn_train_set.npy', hn_train_set)\n",
    "np.save('ln_train_set.npy', ln_train_set)\n",
    "\n",
    "np.save('hn_valid_set.npy', hn_valid_set)\n",
    "np.save('ln_valid_set.npy', ln_valid_set)\n",
    "\n",
    "np.save('hn_test_set.npy', hn_test_set)\n",
    "np.save('ln_test_set.npy', ln_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a53b29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#test whether any indices are shared between datasets\n",
    "\n",
    "print(np.max(np.in1d(train_indices,valid_indices)))\n",
    "print(np.max(np.in1d(train_indices,test_indices)))\n",
    "print(np.max(np.in1d(valid_indices,test_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64bb264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hn = train_indices[:int(round(len(train_indices)/2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c8845a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = train_indices[int(round(len(train_indices)/2)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "919a4d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.max(np.in1d(hn,ln)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "379539c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52ae1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
