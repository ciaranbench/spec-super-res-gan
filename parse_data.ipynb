{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc58bb0",
   "metadata": {},
   "source": [
    "# Parsing training and validations sets + saving test set\n",
    "\n",
    "https://github.com/conor-horgan/DeepeR has a link to a dataset of spectra. \n",
    "\n",
    "The database is composed of paired examples (noisy and high SNR versions of the same spectra)\n",
    "\n",
    "Here, we parse this dataset to train our unsupervised spectral denoising framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4bfac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .mat files are nto in hdf5 format for some reason, so we use scipy.io to read them...\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import random\n",
    "\n",
    "#set how much of the training data to split for train and valid\n",
    "train_split = 0.8\n",
    "validation_split = 0.2\n",
    "assert((train_split + validation_split) <= 1)\n",
    "#use this if we'd like plit across the combined set formed by their train/test files\n",
    "#test_split = 1 - (train_split + validation_split)\n",
    "\n",
    "hn_train = scipy.io.loadmat('Train_Inputs.mat')\n",
    "hn_train = hn_train['Train_Inputs']\n",
    "ln_train = scipy.io.loadmat('Train_Outputs.mat')\n",
    "ln_train = ln_train['Train_Outputs']\n",
    "\n",
    "hn_test = scipy.io.loadmat('Test_Inputs.mat')\n",
    "hn_test = hn_test['Test_Inputs']\n",
    "ln_test = scipy.io.loadmat('Test_Outputs.mat')\n",
    "ln_test = ln_test['Test_Outputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c3b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this if we'd like plit across the combined set formed by their train/test files\n",
    "#hn_all = np.concatenate((hn_train,hn_test), axis=0)\n",
    "#ln_all = np.concatenate((ln_train,ln_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b2d4850",
   "metadata": {},
   "outputs": [],
   "source": [
    "hn_all = hn_train\n",
    "ln_all = ln_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91e820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c54fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomise the order of spectra by getting a random set of indices\n",
    "randomised_indices = random.sample(range(0, np.round(np.shape(hn_all)[0])), int(np.floor(np.shape(hn_all)[0])))\n",
    "# split these indices to define the training and validation examples\n",
    "train_indices = randomised_indices[:int(np.round(train_split*np.shape(hn_all)[0]))]\n",
    "valid_indices = randomised_indices[int(np.round(train_split*np.shape(hn_all)[0])):(int(np.round(train_split*np.shape(hn_all)[0]))+int(np.round(validation_split*np.shape(hn_all)[0])))]\n",
    "#use this if we'd like split across the combined set formed by their train/test files\n",
    "#test_indices = randomised_indices[(int(np.round(train_split*np.shape(hn_all)[0]))+int(np.round(validation_split*np.shape(hn_all)[0]))):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba8c40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the training and validation examples. \n",
    "# the training set is unpaired, so we only use half the examples for each spectra type\n",
    "hn_train_set = hn_all[train_indices[:int(round(len(train_indices)/2))]]\n",
    "ln_train_set = ln_all[train_indices[int(round(len(train_indices)/2)):]]\n",
    "\n",
    "# create an unpaired validation set \n",
    "hn_valid_set = hn_all[valid_indices[:int(round(len(valid_indices)/2))]]\n",
    "ln_valid_set = ln_all[valid_indices[int(round(len(valid_indices)/2)):]]\n",
    "\n",
    "# create a paired version of the validation set, using the same number of examples/indices as \n",
    "# the low snr unpaired validation data to enable a comparison of a supervised validation loss \n",
    "# with the unsupervised validation loss\n",
    "hn_valid_set_sup = hn_all[valid_indices[:int(round(len(valid_indices)/2))]]\n",
    "ln_valid_set_sup = ln_all[valid_indices[:int(round(len(valid_indices)/2))]]\n",
    "\n",
    "\n",
    "#hn_test_set = hn_all[test_indices]\n",
    "#ln_test_set = ln_all[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a7d3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use their original test set as ours. \n",
    "hn_test_set = hn_test\n",
    "ln_test_set = ln_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0325945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('hn_train_set.npy', hn_train_set)\n",
    "np.save('ln_train_set.npy', ln_train_set)\n",
    "\n",
    "np.save('hn_valid_set.npy', hn_valid_set)\n",
    "np.save('ln_valid_set.npy', ln_valid_set)\n",
    "\n",
    "np.save('hn_test_set.npy', hn_test_set)\n",
    "np.save('ln_test_set.npy', ln_test_set)\n",
    "\n",
    "np.save('hn_valid_set_sup.npy', hn_valid_set_sup)\n",
    "np.save('ln_valid_set_sup.npy', ln_valid_set_sup)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a53b29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#test whether any indices are shared between datasets\n",
    "print(np.max(np.in1d(train_indices,valid_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64bb264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hn = train_indices[:int(round(len(train_indices)/2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c8845a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = train_indices[int(round(len(train_indices)/2)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "919a4d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.max(np.in1d(hn,ln)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379539c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52ae1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
