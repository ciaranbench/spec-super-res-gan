{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e244c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e5373a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-13 20:33:51.480877: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-13 20:33:51.514953: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-13 20:33:52.052914: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Version:  2.12.0\n",
      "Python Version:  3.10.6\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow_probability as tfp\n",
    "\n",
    "import tensorflow as tf\n",
    "print('TF Version: ', tf.__version__)\n",
    "from platform import python_version\n",
    "print('Python Version: ', python_version())\n",
    "\n",
    "# Add path to src that contains files needed to construct network/training procedure.\n",
    "import sys\n",
    "#sys.path.insert(0, './src')\n",
    "sys.path.append('src')\n",
    "\n",
    "# Install the receptive_field pip package in the Jupyter kernel\n",
    "# https://github.com/google-research/receptive_field\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install src/receptive_field/.\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f8a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = 1 # define the GPU to use\n",
    "# Set the GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU)\n",
    "#from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cae147b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ciaran/.local/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/ciaran/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-13 20:33:53.718437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22071 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:4e:00.0, compute capability: 8.9\n",
      "/home/ciaran/.local/lib/python3.10/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Title: CycleGAN\n",
    "Author: [A_K_Nain](https://twitter.com/A_K_Nain)\n",
    "Date created: 2020/08/12\n",
    "Last modified: 2020/08/12\n",
    "Description: Implementation of CycleGAN.\n",
    "Accelerator: GPU\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## CycleGAN\n",
    "\n",
    "CycleGAN is a model that aims to solve the image-to-image translation\n",
    "problem. The goal of the image-to-image translation problem is to learn the\n",
    "mapping between an input image and an output image using a training set of\n",
    "aligned image pairs. However, obtaining paired examples isn't always feasible.\n",
    "CycleGAN tries to learn this mapping without requiring paired input-output images,\n",
    "using cycle-consistent adversarial networks.\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/1703.10593.pdf)\n",
    "- [Original implementation](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## Setup\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "autotune = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Prepare the dataset\n",
    "\n",
    "In this example, we will be using the\n",
    "[horse to zebra](https://www.tensorflow.org/datasets/catalog/cycle_gan#cycle_ganhorse2zebra)\n",
    "dataset.\n",
    "\"\"\"\n",
    "\n",
    "# Load the horse-zebra dataset using tensorflow-datasets.\n",
    "#dataset, _ = tfds.load(\"cycle_gan/horse2zebra\", with_info=True, as_supervised=True)\n",
    "noisy_tr = np.load('hn_train_set.npy')\n",
    "clean_tr = np.load('ln_train_set.npy')\n",
    "noisy_te = np.load('hn_test_set.npy')\n",
    "clean_te = np.load('ln_test_set.npy')\n",
    "#train_horses, train_zebras = np.expand_dims(noisy_tr[:200],axis=0), np.expand_dims(clean_tr[:200],axis=0)\n",
    "#test_horses, test_zebras = np.expand_dims(noisy_tr[:1],axis=0), np.expand_dims(clean_tr[:1],axis=0)\n",
    "train_horses, train_zebras = noisy_tr[:200], clean_tr[:200]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the standard image size.\n",
    "orig_img_size = (500)\n",
    "# Size of the random crops to be used during training.\n",
    "input_img_size = (500,1)\n",
    "# Weights initializer for the layers.\n",
    "kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "# Gamma initializer for instance normalization.\n",
    "gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "buffer_size = 256\n",
    "batch_size = 5\n",
    "\n",
    "\n",
    "def normalize_img(img):\n",
    "    img = tf.cast(img, dtype=tf.float32)\n",
    "    # Map values in the range [-1, 1]\n",
    "    return (img / 127.5) - 1.0\n",
    "\n",
    "\n",
    "def preprocess_train_image(img, label):\n",
    "    # Random flip\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    # Resize to the original size first\n",
    "    img = tf.image.resize(img, [*orig_img_size])\n",
    "    # Random crop to 256X256\n",
    "    img = tf.image.random_crop(img, size=[*input_img_size])\n",
    "    # Normalize the pixel values in the range [-1, 1]\n",
    "    img = normalize_img(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_test_image(img, label):\n",
    "    # Only resizing and normalization for the test images.\n",
    "    img = tf.image.resize(img, [input_img_size[0], input_img_size[1]])\n",
    "    img = normalize_img(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Create `Dataset` objects\n",
    "\"\"\"\n",
    "'''\n",
    "train_horses = tf.data.Dataset.from_tensor_slices((train_horses))\n",
    "train_zebra = tf.data.Dataset.from_tensor_slices((train_zebras))\n",
    "test_horses = tf.data.Dataset.from_tensor_slices((test_horses))\n",
    "test_zebra = tf.data.Dataset.from_tensor_slices((test_zebras))\n",
    "'''\n",
    "\n",
    "'''\n",
    "# Apply the preprocessing operations to the training data\n",
    "train_horses = (\n",
    "    train_horses.map(preprocess_train_image, num_parallel_calls=autotune)\n",
    "    .cache()\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size)\n",
    ")\n",
    "train_zebras = (\n",
    "    train_zebras.map(preprocess_train_image, num_parallel_calls=autotune)\n",
    "    .cache()\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size)\n",
    ")\n",
    "\n",
    "# Apply the preprocessing operations to the test data\n",
    "test_horses = (\n",
    "    test_horses.map(preprocess_test_image, num_parallel_calls=autotune)\n",
    "    .cache()\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size)\n",
    ")\n",
    "test_zebras = (\n",
    "    test_zebras.map(preprocess_test_image, num_parallel_calls=autotune)\n",
    "    .cache()\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size)\n",
    ")\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "## Visualize some samples\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "_, ax = plt.subplots(4, 2, figsize=(10, 15))\n",
    "for i, samples in enumerate(zip(train_horses.take(4), train_zebras.take(4))):\n",
    "    horse = (((samples[0][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n",
    "    zebra = (((samples[1][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n",
    "    ax[i, 0].plot(horse)\n",
    "    ax[i, 1].plot(zebra)\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "## Building blocks used in the CycleGAN generators and discriminators\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ReflectionPadding2D(layers.Layer):\n",
    "    \"\"\"Implements Reflection Padding as a layer.\n",
    "\n",
    "    Args:\n",
    "        padding(tuple): Amount of padding for the\n",
    "        spatial dimensions.\n",
    "\n",
    "    Returns:\n",
    "        A padded tensor with the same type as the input tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, input_tensor, mask=None):\n",
    "        padding_width, padding_height = self.padding\n",
    "        padding_tensor = [\n",
    "            [0, 0],\n",
    "            [padding_height, padding_height],\n",
    "            [padding_width, padding_width],\n",
    "            [0, 0],\n",
    "        ]\n",
    "        return tf.pad(input_tensor, padding_tensor, mode=\"REFLECT\")\n",
    "\n",
    "\n",
    "def residual_block(\n",
    "    x,\n",
    "    activation,\n",
    "    kernel_initializer=kernel_init,\n",
    "    kernel_size=(3),\n",
    "    strides=(1),\n",
    "    padding=\"same\",\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    dim = x.shape[-1]\n",
    "    input_tensor = x\n",
    "\n",
    "    #x = ReflectionPadding2D()(input_tensor)\n",
    "    x = layers.Conv1D(\n",
    "        dim,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(input_tensor)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = activation(x)\n",
    "\n",
    "    #x = ReflectionPadding2D()(x)\n",
    "    x = layers.Conv1D(\n",
    "        dim,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = layers.add([input_tensor, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def downsample(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_initializer=kernel_init,\n",
    "    kernel_size=(3),\n",
    "    strides=(2),\n",
    "    padding=\"same\",\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv1D(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def upsample(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3),\n",
    "    strides=(2),\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=kernel_init,\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv1DTranspose(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Build the generators\n",
    "\n",
    "The generator consists of downsampling blocks: nine residual blocks\n",
    "and upsampling blocks. The structure of the generator is the following:\n",
    "\n",
    "```\n",
    "c7s1-64 ==> Conv block with `relu` activation, filter size of 7\n",
    "d128 ====|\n",
    "         |-> 2 downsampling blocks\n",
    "d256 ====|\n",
    "R256 ====|\n",
    "R256     |\n",
    "R256     |\n",
    "R256     |\n",
    "R256     |-> 9 residual blocks\n",
    "R256     |\n",
    "R256     |\n",
    "R256     |\n",
    "R256 ====|\n",
    "u128 ====|\n",
    "         |-> 2 upsampling blocks\n",
    "u64  ====|\n",
    "c7s1-3 => Last conv block with `tanh` activation, filter size of 7.\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_resnet_generator(\n",
    "    filters=64,\n",
    "    num_downsampling_blocks=2,\n",
    "    num_residual_blocks=9,\n",
    "    num_upsample_blocks=2,\n",
    "    gamma_initializer=gamma_init,\n",
    "    name=None,\n",
    "):\n",
    "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
    "    #x = ReflectionPadding2D(padding=(3, 3))(img_input)\n",
    "    x = layers.Conv1D(filters, (7), kernel_initializer=kernel_init, use_bias=False,padding=\"same\")(\n",
    "        img_input\n",
    "    )\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    # Downsampling\n",
    "    for _ in range(num_downsampling_blocks):\n",
    "        filters *= 2\n",
    "        x = downsample(x, filters=filters, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Residual blocks\n",
    "    for _ in range(num_residual_blocks):\n",
    "        x = residual_block(x, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Upsampling\n",
    "    for _ in range(num_upsample_blocks):\n",
    "        filters //= 2\n",
    "        x = upsample(x, filters, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Final block\n",
    "    #x = ReflectionPadding2D(padding=(3))(x)\n",
    "    x = layers.Conv1D(1, (7), padding=\"same\")(x)\n",
    "    x = layers.Activation(\"tanh\")(x)\n",
    "\n",
    "    model = keras.models.Model(img_input, x, name=name)\n",
    "    return model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Build the discriminators\n",
    "\n",
    "The discriminators implement the following architecture:\n",
    "`C64->C128->C256->C512`\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_discriminator(\n",
    "    filters=64, kernel_initializer=kernel_init, num_downsampling=3, name=None\n",
    "):\n",
    "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
    "    x = layers.Conv1D(\n",
    "        filters,\n",
    "        (4),\n",
    "        strides=(2),\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )(img_input)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    num_filters = filters\n",
    "    for num_downsample_block in range(3):\n",
    "        num_filters *= 2\n",
    "        if num_downsample_block < 2:\n",
    "            x = downsample(\n",
    "                x,\n",
    "                filters=num_filters,\n",
    "                activation=layers.LeakyReLU(0.2),\n",
    "                kernel_size=(4),\n",
    "                strides=(2),\n",
    "            )\n",
    "        else:\n",
    "            x = downsample(\n",
    "                x,\n",
    "                filters=num_filters,\n",
    "                activation=layers.LeakyReLU(0.2),\n",
    "                kernel_size=(4),\n",
    "                strides=(1),\n",
    "            )\n",
    "\n",
    "    x = layers.Conv1D(\n",
    "        1, (4), strides=(1), padding=\"same\", kernel_initializer=kernel_initializer\n",
    "    )(x)\n",
    "\n",
    "    model = keras.models.Model(inputs=img_input, outputs=x, name=name)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get the generators\n",
    "gen_G = get_resnet_generator(name=\"generator_G\")\n",
    "gen_F = get_resnet_generator(name=\"generator_F\")\n",
    "\n",
    "# Get the discriminators\n",
    "disc_X = get_discriminator(name=\"discriminator_X\")\n",
    "disc_Y = get_discriminator(name=\"discriminator_Y\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Build the CycleGAN model\n",
    "\n",
    "We will override the `train_step()` method of the `Model` class\n",
    "for training via `fit()`.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CycleGan(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator_G,\n",
    "        generator_F,\n",
    "        discriminator_X,\n",
    "        discriminator_Y,\n",
    "        lambda_cycle=10.0,\n",
    "        lambda_identity=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gen_G = generator_G\n",
    "        self.gen_F = generator_F\n",
    "        self.disc_X = discriminator_X\n",
    "        self.disc_Y = discriminator_Y\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        self.lambda_identity = lambda_identity\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return (\n",
    "            self.disc_X(inputs),\n",
    "            self.disc_Y(inputs),\n",
    "            self.gen_G(inputs),\n",
    "            self.gen_F(inputs),\n",
    "        )\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        gen_G_optimizer,\n",
    "        gen_F_optimizer,\n",
    "        disc_X_optimizer,\n",
    "        disc_Y_optimizer,\n",
    "        gen_loss_fn,\n",
    "        disc_loss_fn,\n",
    "    ):\n",
    "        super().compile()\n",
    "        self.gen_G_optimizer = gen_G_optimizer\n",
    "        self.gen_F_optimizer = gen_F_optimizer\n",
    "        self.disc_X_optimizer = disc_X_optimizer\n",
    "        self.disc_Y_optimizer = disc_Y_optimizer\n",
    "        self.generator_loss_fn = gen_loss_fn\n",
    "        self.discriminator_loss_fn = disc_loss_fn\n",
    "        self.cycle_loss_fn = keras.losses.MeanAbsoluteError()\n",
    "        self.identity_loss_fn = keras.losses.MeanAbsoluteError()\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        # x is Horse and y is zebra\n",
    "        real_x, real_y = batch_data\n",
    "\n",
    "        # For CycleGAN, we need to calculate different\n",
    "        # kinds of losses for the generators and discriminators.\n",
    "        # We will perform the following steps here:\n",
    "        #\n",
    "        # 1. Pass real images through the generators and get the generated images\n",
    "        # 2. Pass the generated images back to the generators to check if we\n",
    "        #    we can predict the original image from the generated image.\n",
    "        # 3. Do an identity mapping of the real images using the generators.\n",
    "        # 4. Pass the generated images in 1) to the corresponding discriminators.\n",
    "        # 5. Calculate the generators total loss (adverserial + cycle + identity)\n",
    "        # 6. Calculate the discriminators loss\n",
    "        # 7. Update the weights of the generators\n",
    "        # 8. Update the weights of the discriminators\n",
    "        # 9. Return the losses in a dictionary\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Horse to fake zebra\n",
    "            fake_y = self.gen_G(real_x, training=True)\n",
    "            # Zebra to fake horse -> y2x\n",
    "            fake_x = self.gen_F(real_y, training=True)\n",
    "\n",
    "            # Cycle (Horse to fake zebra to fake horse): x -> y -> x\n",
    "            cycled_x = self.gen_F(fake_y, training=True)\n",
    "            # Cycle (Zebra to fake horse to fake zebra) y -> x -> y\n",
    "            cycled_y = self.gen_G(fake_x, training=True)\n",
    "\n",
    "            # Identity mapping\n",
    "            same_x = self.gen_F(real_x, training=True)\n",
    "            same_y = self.gen_G(real_y, training=True)\n",
    "\n",
    "            # Discriminator output\n",
    "            disc_real_x = self.disc_X(real_x, training=True)\n",
    "            disc_fake_x = self.disc_X(fake_x, training=True)\n",
    "\n",
    "            disc_real_y = self.disc_Y(real_y, training=True)\n",
    "            disc_fake_y = self.disc_Y(fake_y, training=True)\n",
    "\n",
    "            # Generator adverserial loss\n",
    "            gen_G_loss = self.generator_loss_fn(disc_fake_y)\n",
    "            gen_F_loss = self.generator_loss_fn(disc_fake_x)\n",
    "\n",
    "            # Generator cycle loss\n",
    "            cycle_loss_G = self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle\n",
    "            cycle_loss_F = self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle\n",
    "\n",
    "            # Generator identity loss\n",
    "            id_loss_G = (\n",
    "                self.identity_loss_fn(real_y, same_y)\n",
    "                * self.lambda_cycle\n",
    "                * self.lambda_identity\n",
    "            )\n",
    "            id_loss_F = (\n",
    "                self.identity_loss_fn(real_x, same_x)\n",
    "                * self.lambda_cycle\n",
    "                * self.lambda_identity\n",
    "            )\n",
    "\n",
    "            # Total generator loss\n",
    "            total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G\n",
    "            total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F\n",
    "\n",
    "            # Discriminator loss\n",
    "            disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)\n",
    "            disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)\n",
    "\n",
    "        # Get the gradients for the generators\n",
    "        grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\n",
    "        grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\n",
    "\n",
    "        # Get the gradients for the discriminators\n",
    "        disc_X_grads = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n",
    "        disc_Y_grads = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n",
    "\n",
    "        # Update the weights of the generators\n",
    "        self.gen_G_optimizer.apply_gradients(\n",
    "            zip(grads_G, self.gen_G.trainable_variables)\n",
    "        )\n",
    "        self.gen_F_optimizer.apply_gradients(\n",
    "            zip(grads_F, self.gen_F.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Update the weights of the discriminators\n",
    "        self.disc_X_optimizer.apply_gradients(\n",
    "            zip(disc_X_grads, self.disc_X.trainable_variables)\n",
    "        )\n",
    "        self.disc_Y_optimizer.apply_gradients(\n",
    "            zip(disc_Y_grads, self.disc_Y.trainable_variables)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"G_loss\": total_loss_G,\n",
    "            \"F_loss\": total_loss_F,\n",
    "            \"D_X_loss\": disc_X_loss,\n",
    "            \"D_Y_loss\": disc_Y_loss,\n",
    "        }\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Create a callback that periodically saves generated images\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate and save images after each epoch\"\"\"\n",
    "\n",
    "    def __init__(self, num_img=2):\n",
    "        self.num_img = num_img\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        _, ax = plt.subplots(2, 1, figsize=(12, 12))\n",
    "        img = np.reshape(test_horses,(500,1))\n",
    "        img = np.expand_dims(img,axis=0)\n",
    "        prediction = self.model.gen_G(img)\n",
    "        ax[0].plot(np.squeeze(img))\n",
    "        ax[1].plot(np.squeeze(prediction))\n",
    "        '''\n",
    "        for i, img in enumerate(test_horses.take(self.num_img)):\n",
    "            prediction = self.model.gen_G(img)[0].numpy()\n",
    "            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
    "            img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
    "\n",
    "            ax[i, 0].imshow(img)\n",
    "            ax[i, 1].imshow(prediction)\n",
    "            ax[i, 0].set_title(\"Input image\")\n",
    "            ax[i, 1].set_title(\"Translated image\")\n",
    "            ax[i, 0].axis(\"off\")\n",
    "            ax[i, 1].axis(\"off\")\n",
    "\n",
    "            prediction = keras.preprocessing.image.array_to_img(prediction)\n",
    "            prediction.save(\n",
    "                \"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch + 1)\n",
    "            )\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        '''\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Train the end-to-end model\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Loss function for evaluating adversarial loss\n",
    "adv_loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define the loss function for the generators\n",
    "\n",
    "\n",
    "def generator_loss_fn(fake):\n",
    "    fake_loss = adv_loss_fn(tf.ones_like(fake), fake)\n",
    "    return fake_loss\n",
    "\n",
    "\n",
    "# Define the loss function for the discriminators\n",
    "def discriminator_loss_fn(real, fake):\n",
    "    real_loss = adv_loss_fn(tf.ones_like(real), real)\n",
    "    fake_loss = adv_loss_fn(tf.zeros_like(fake), fake)\n",
    "    return (real_loss + fake_loss) * 0.5\n",
    "\n",
    "\n",
    "# Create cycle gan model\n",
    "cycle_gan_model = CycleGan(\n",
    "    generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "cycle_gan_model.compile(\n",
    "    gen_G_optimizer=keras.optimizers.legacy.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    gen_F_optimizer=keras.optimizers.legacy.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    disc_X_optimizer=keras.optimizers.legacy.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    disc_Y_optimizer=keras.optimizers.legacy.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    gen_loss_fn=generator_loss_fn,\n",
    "    disc_loss_fn=discriminator_loss_fn,\n",
    ")\n",
    "# Callbacks\n",
    "plotter = GANMonitor()\n",
    "checkpoint_filepath = \"./model_checkpoints/cyclegan_checkpoints.{epoch:03d}\"\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath, save_weights_only=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d99f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176bdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a33709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d7170b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-13 20:33:55.052509: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_5' with dtype double and shape [200,500]\n",
      "\t [[{{node Placeholder/_5}}]]\n",
      "2023-05-13 20:33:55.052634: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_5' with dtype double and shape [200,500]\n",
      "\t [[{{node Placeholder/_5}}]]\n",
      "2023-05-13 20:34:15.787048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - ETA: 0s - G_loss: 0.6516 - F_loss: 0.7309 - D_X_loss: 0.2894 - D_Y_loss: 0.2894"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_horses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m train_horses \u001b[38;5;241m=\u001b[39m train_horses\u001b[38;5;241m.\u001b[39mbatch(batch_size)\n\u001b[1;32m      6\u001b[0m train_zebras \u001b[38;5;241m=\u001b[39m train_zebras\u001b[38;5;241m.\u001b[39mbatch(batch_size)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mcycle_gan_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_horses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_zebras\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mplotter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mTest the performance of the model.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03mYou can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/CycleGAN)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03mand try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/CycleGAN).\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# This model was trained for 90 epochs. We will be loading those weights\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# here. Once the weights are loaded, we will take a few samples from the test\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# data and check the model's performance.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[3], line 567\u001b[0m, in \u001b[0;36mGANMonitor.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_epoch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, epoch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    566\u001b[0m     _, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m12\u001b[39m))\n\u001b[0;32m--> 567\u001b[0m     img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(\u001b[43mtest_horses\u001b[49m,(\u001b[38;5;241m500\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    568\u001b[0m     img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    569\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgen_G(img)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_horses' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_horses = tf.data.Dataset.from_tensor_slices((train_horses))\n",
    "train_zebras = tf.data.Dataset.from_tensor_slices((train_zebras))\n",
    "#test_horses = tf.data.Dataset.from_tensor_slices((test_horses))\n",
    "#test_zebras = tf.data.Dataset.from_tensor_slices((test_zebras))\n",
    "train_horses = train_horses.batch(batch_size)\n",
    "train_zebras = train_zebras.batch(batch_size)\n",
    "\n",
    "\n",
    "\n",
    "cycle_gan_model.fit(\n",
    "    tf.data.Dataset.zip((train_horses, train_zebras)),\n",
    "    epochs=1,\n",
    "    callbacks=[plotter, model_checkpoint_callback],\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Test the performance of the model.\n",
    "\n",
    "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/CycleGAN)\n",
    "and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/CycleGAN).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# This model was trained for 90 epochs. We will be loading those weights\n",
    "# here. Once the weights are loaded, we will take a few samples from the test\n",
    "# data and check the model's performance.\n",
    "\n",
    "\"\"\"shell\n",
    "curl -LO https://github.com/AakashKumarNain/CycleGAN_TF2/releases/download/v1.0/saved_checkpoints.zip\n",
    "unzip -qq saved_checkpoints.zip\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zebras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc377b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9f7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the checkpoints\n",
    "weight_file = \"./saved_checkpoints/cyclegan_checkpoints.001\"\n",
    "cycle_gan_model.load_weights(weight_file).expect_partial()\n",
    "print(\"Weights loaded successfully\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
